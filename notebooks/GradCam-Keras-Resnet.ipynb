{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "BASE_PATH = \"/home/MD00560695/workdir/gradcam\"\n",
    "sys.path.append(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(BASE_PATH, \"data/raw/tiny-imagenet-200/\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Download the data\n",
    "! cd data/raw\n",
    "! wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "! unzip tiny-imagenet-200.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = os.path.join(DATA_PATH, \"val/images/val_10.JPEG\") #\"val/cat_dog.png\")\n",
    "SAMPLE_DIR = os.path.join(DATA_PATH, \"val/images\") # \"imgs/samples\")\n",
    "MODEL_DIR= os.path.join(BASE_PATH, \"models\")\n",
    "logo_img = os.path.join(BASE_PATH, \"imgs\", \"alaiom_07.JPG\")\n",
    "LOG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 % 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "# Models architecture\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Javascript, HTML, clear_output, IFrame\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, AppLayout, GridspecLayout\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = (224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, preprocess=True):\n",
    "    \"\"\"Load and preprocess image.\"\"\"\n",
    "    x = image.load_img(path, target_size=(H, W))\n",
    "    if preprocess:\n",
    "        x = image.img_to_array(x)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_image(x):\n",
    "    \"\"\"Same normalization as in:\n",
    "    https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n",
    "    \"\"\"\n",
    "    # normalize tensor: center on 0., ensure std is 0.25\n",
    "    x = x.copy()\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + K.epsilon())\n",
    "    x *= 0.25\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess(x):\n",
    "    if np.ndim(x) > 3:\n",
    "        x = np.squeeze(x)\n",
    "        \n",
    "    with tf.GradientTape() as tape:\n",
    "        x = -tf.math.reduce_mean(x)\n",
    "        x /= (tf.math.reduce_std(x) + 1e-5) \n",
    "        x *= 0.1\n",
    "\n",
    "        # clip to [0, 1]\n",
    "        x += 0.5\n",
    "        x = np.clip(x, 0, 1)\n",
    "\n",
    "        # convert to RGB array\n",
    "        x *= 25\n",
    "        if K.image_data_format() == 'th':\n",
    "            x = x.transpose((1, 2, 0))\n",
    "            \n",
    "        x = np.clip(x, 0, 255).astype('uint8')\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"Utility function to normalize a tensor by its L2 norm\"\"\"\n",
    "    return (x + 1e-10) / (K.sqrt(K.mean(K.square(x))) + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array2bytes(im_arr, fmt='png'):\n",
    "    img = Image.fromarray(im_arr, mode='RGB')\n",
    "    f = BytesIO()\n",
    "    img.save(f, fmt)\n",
    "\n",
    "    return f.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename):\n",
    "    im = img_to_array(load_img(os.path.join(SAMPLE_DIR,filename),target_size = TARGET_SIZE))\n",
    "    x = np.expand_dims(im, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanilaResNet50:\n",
    "    def __init__(self, n_classes=1000):\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def __call__(self):\n",
    "        resnet = ResNet50V2(include_top=False, pooling=\"avg\", weights='imagenet')\n",
    "        for layer in resnet.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        logits = Dense(self.n_classes)(resnet.layers[-1].output)\n",
    "        output = Activation('softmax')(logits)\n",
    "        model = Model(resnet.input, output)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50PlusFC:\n",
    "    def __init__(self, n_classes=1000):\n",
    "        resnet = ResNet50V2(include_top=False, pooling=\"avg\", weights='imagenet')\n",
    "        for layer in resnet.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        fc1 = Dense(100)(resnet.layers[-1].output)\n",
    "        fc2 = Dense(100)(fc1)\n",
    "        logits = Dense(n_classes)(fc2)\n",
    "        output = Activation('softmax')(logits)\n",
    "        model = Model(resnet.input, output)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_VanilaResNet50(n_classes=1000):\n",
    "    resnet = ResNet50V2(include_top=False, pooling=\"avg\", weights='imagenet')\n",
    "    for layer in resnet.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    logits = Dense(n_classes)(resnet.layers[-1].output)\n",
    "    output = Activation('softmax')(logits)\n",
    "    model = Model(resnet.input, output)\n",
    "    #model.load_weights(\"{}/resnet50best.hdf5\".format(MODEL_DIR))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ResNet50PlusFC(n_classes=1000):\n",
    "    resnet = ResNet50V2(include_top=False, pooling=\"avg\", weights='imagenet')\n",
    "    for layer in resnet.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    fc1 = Dense(100)(resnet.layers[-1].output)\n",
    "    fc2 = Dense(100)(fc1)\n",
    "    logits = Dense(n_classes)(fc2)\n",
    "    output = Activation('softmax')(logits)\n",
    "    model = Model(resnet.input, output)\n",
    "    #model.load_weights(\"{}/resnet50fcbest.hdf5\".format(MODEL_DIR))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ResNet50():\n",
    "    return ResNet50(include_top=True, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, processed_im):\n",
    "    top_n = 5\n",
    "    preds = model.predict(processed_im)\n",
    "    \n",
    "    top_pred_n = decode_predictions(preds, top=top_n)[0]\n",
    "    classes = np.argsort(preds[0])[-top_n:][::-1]\n",
    "        \n",
    "    P = imagenet_utils.decode_predictions(preds)\n",
    "    idx = preds.argmax()\n",
    "    return idx, preds.max(), classes, top_pred_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRAD CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    # Adapted with some modification from \n",
    "    # https://www.pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/\n",
    "    def __init__(self, model, layerName=None):\n",
    "        \"\"\"\n",
    "        model: pre-softmax layer (logit layer)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.layerName = layerName\n",
    "        self.cam3 = None\n",
    "\n",
    "        if self.layerName == None:\n",
    "            self.layerName = self.find_target_layer()\n",
    "\n",
    "    def find_target_layer(self):\n",
    "        for layer in reversed(self.model.layers):\n",
    "            if len(layer.output_shape) == 4:\n",
    "                return layer.name\n",
    "        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM\")\n",
    "\n",
    "    def compute_heatmap(self, image, classIdx, upsample_size, eps=1e-5):\n",
    "        gradModel = Model(\n",
    "            inputs=[self.model.inputs],\n",
    "            outputs=[self.model.get_layer(self.layerName).output, self.model.output]\n",
    "        )\n",
    "        # record operations for automatic differentiation\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            inputs = tf.cast(image, tf.float32)\n",
    "            (convOuts, preds) = gradModel(inputs)  # preds after softmax\n",
    "            loss = preds[:, classIdx]\n",
    "\n",
    "        # compute gradients with automatic differentiation\n",
    "        grads = tape.gradient(loss, convOuts)\n",
    "        # discard batch\n",
    "        convOuts = convOuts[0]\n",
    "        grads = grads[0]\n",
    "        norm_grads = tf.divide(grads, tf.reduce_mean(tf.square(grads)) + tf.constant(eps))\n",
    "\n",
    "        # compute weights\n",
    "        weights = tf.reduce_mean(norm_grads, axis=(0, 1))\n",
    "        cam = tf.reduce_sum(tf.multiply(weights, convOuts), axis=-1)\n",
    "\n",
    "        # Apply reLU\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = cam / np.max(cam)\n",
    "        cam = cv2.resize(cam, upsample_size,cv2.INTER_LINEAR)\n",
    "\n",
    "        # convert to 3D\n",
    "        cam3 = np.expand_dims(cam, axis=2)\n",
    "        cam3 = np.tile(cam3, [1, 1, 3])\n",
    "        \n",
    "        self.cam3 = cam3\n",
    "        return cam3\n",
    "    \n",
    "    \n",
    "    def overlay_gradCAM(self, img):\n",
    "        superimposed_cam3 = np.uint8(255 * self.cam3)\n",
    "        superimposed_cam3 = cv2.applyColorMap(superimposed_cam3, cv2.COLORMAP_JET)\n",
    "\n",
    "        superimposed_img = 0.3 * superimposed_cam3 + 0.5 * img\n",
    "\n",
    "        return (superimposed_img * 255.0 / superimposed_img.max()).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Backpropogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def guidedRelu(x):\n",
    "    def grad(dy):\n",
    "        return tf.cast(dy>0,\"float32\") * tf.cast(x>0, \"float32\") * dy\n",
    "    return tf.nn.relu(x), grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference: https://github.com/eclique/keras-gradcam with adaption to tensorflow 2.0  \n",
    "class GuidedBackprop:\n",
    "    def __init__(self,model, layerName=None):\n",
    "        self.model = model\n",
    "        self.layerName = layerName\n",
    "        if self.layerName == None:\n",
    "            self.layerName = self.find_target_layer()\n",
    "        self.gbModel = self.build_guided_model()\n",
    "        \n",
    "    def find_target_layer(self):\n",
    "        for layer in reversed(self.model.layers):\n",
    "            if len(layer.output_shape) == 4:\n",
    "                return layer.name\n",
    "        raise ValueError(\"Could not find 4D layer. Cannot apply Guided Backpropagation\")\n",
    "\n",
    "    def build_guided_model(self):\n",
    "        gbModel = Model(\n",
    "            inputs = [self.model.inputs],\n",
    "            outputs = [self.model.get_layer(self.layerName).output]\n",
    "        )\n",
    "        layer_dict = [layer for layer in gbModel.layers[1:] if hasattr(layer,\"activation\")]\n",
    "        for layer in layer_dict:\n",
    "            if layer.activation == tf.keras.activations.relu:\n",
    "                layer.activation = guidedRelu\n",
    "        \n",
    "        return gbModel\n",
    "    \n",
    "    def guided_backprop(self, images, upsample_size):\n",
    "        \"\"\"Guided Backpropagation method for visualizing input saliency.\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            inputs = tf.cast(images, tf.float32)\n",
    "            tape.watch(inputs)\n",
    "            outputs = self.gbModel(inputs)\n",
    "\n",
    "        grads = tape.gradient(outputs, inputs)[0]\n",
    "\n",
    "        saliency = cv2.resize(np.asarray(grads), upsample_size)\n",
    "\n",
    "        return saliency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build IPywidget UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header\n",
    "header = widgets.HTML('<font color=\"#1f77b4\" face=\"sans-serif\"><center><h1>DEMO GradCAM and Guided GradCAM</h1></center></font>',\n",
    "                      layout=widgets.Layout(height='auto'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logo\n",
    "logo = widgets.Image(\n",
    "    value=open(logo_img, \"rb\").read(),\n",
    "    format='png',\n",
    "    width='auto',\n",
    "    height='auto',\n",
    "    align=\"center-align\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setclasseswidget(class_ls):\n",
    "    CLASSES = widgets.Dropdown(options=class_ls, description=\"Class\", layout={'width':'auto'}, disabled=False)\n",
    "    grid[1,17:24] = CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropdown for Image\n",
    "def on_change_im(change):\n",
    "    if change['type'] == \"change\" and change[\"name\"] == \"value\":\n",
    "        img = cv2.imread(os.path.join(SAMPLE_DIR,change[\"new\"]))\n",
    "        im_arr = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        logo = widgets.Image(\n",
    "            value=array2bytes(im_arr),\n",
    "            format='png',\n",
    "            width='auto',\n",
    "            height='auto',\n",
    "            align=\"center-align\"\n",
    "        )\n",
    "        grid[1,17:24] = widgets.HTML(\"\")\n",
    "        grid[5:13, 1:9] = widgets.HTML(\"\")\n",
    "        grid[5:13, 11:19] = logo\n",
    "        grid[5:13, 21:29] = widgets.HTML(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropdown for model\n",
    "def on_change_model(change):\n",
    "    if change['type'] == \"change\" and change[\"name\"] == \"value\":\n",
    "        chosen_model =  widgets.HTML(\"<center><p>Model %s loaded.<center>\" % change[\"new\"])\n",
    "        grid[2,:8] = chosen_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# button\n",
    "def create_expanded_button(description, button_style):\n",
    "    return widgets.Button(description=description, button_style=button_style,\n",
    "                          layout=widgets.Layout(height='auto', width='auto'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the drop down values\n",
    "im_ls = [\"--Select\"] + os.listdir(SAMPLE_DIR)\n",
    "im_ls.sort()\n",
    "model_ls = [\"--Select\",\"VanilaResNet50\", \"ResNet50PlusFC\"]\n",
    "class_ls = [\"--Select\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the component values\n",
    "CLASSES = widgets.Dropdown(options=class_ls, description=\"Class\", layout={'width':'auto'}, disabled=False)\n",
    "models = widgets.Dropdown(options=model_ls, description=\"Model\",layout={'width':'auto'}, disabled=False)\n",
    "imgs = widgets.Dropdown(options=im_ls, description=\"Image\", layout={'width':'auto'}, disabled=False)\n",
    "imgs.observe(on_change_im)\n",
    "models.observe(on_change_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set button values\n",
    "pred_but = create_expanded_button(\"Show\",\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Grid Layouts\n",
    "grid = GridspecLayout(20, 30, height='700px')\n",
    "grid[0,:] = header\n",
    "grid[1,:8] = models\n",
    "grid[1,8:17] = imgs\n",
    "grid[1,25:] = pred_but\n",
    "grid[5:13,11:19] = logo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functionalities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showCAMs(img, x, GradCAM, GuidedBP, chosen_class, upsample_size):\n",
    "    # Grad CAM\n",
    "    log_info(\"showCAMs\",\"Computing gradcam\")\n",
    "    cam3 = GradCAM.compute_heatmap(image=x, classIdx=chosen_class, upsample_size=upsample_size)\n",
    "    gradcam = GradCAM.overlay_gradCAM(img)\n",
    "    gradcam = cv2.cvtColor(gradcam, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Guided backprop\n",
    "    log_info(\"showCAMs\",\"Computing backprop\")\n",
    "    gb = GuidedBP.guided_backprop(x, upsample_size)\n",
    "    gb_im = deprocess_image(gb)\n",
    "    gb_im = cv2.cvtColor(gb_im, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Guided GradCAM\n",
    "    log_info(\"showCAMs\",\"Computing guidied gradcam\")\n",
    "    guided_gradcam = deprocess_image(gb*cam3)\n",
    "    guided_gradcam = cv2.cvtColor(guided_gradcam, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display\n",
    "    log_info(\"showCAMs\",\"Display images\")\n",
    "    gc = widgets.Image(\n",
    "            value=array2bytes(gradcam),\n",
    "            format='png',\n",
    "            width='auto',\n",
    "            height='auto',\n",
    "            align=\"center-align\"\n",
    "        )\n",
    "    gbim = widgets.Image(\n",
    "            value=array2bytes(gb_im),\n",
    "            format='png',\n",
    "            width='auto',\n",
    "            height='auto',\n",
    "            align=\"center-align\"\n",
    "        )\n",
    "    ggc = widgets.Image(\n",
    "            value=array2bytes(guided_gradcam),\n",
    "            format='png',\n",
    "            width='auto',\n",
    "            height='auto',\n",
    "            align=\"center-align\"\n",
    "        )\n",
    "    \n",
    "    log_info(\"showCAMs\",\"Set new Grid\")\n",
    "    grid[4, 1:9] = widgets.HTML('<center><b>GradCAM</b></center>')\n",
    "    grid[4, 11:19] = widgets.HTML('<center><b>Guided Bacpropagation</b></center>')\n",
    "    grid[4, 21:29] = widgets.HTML('<center><b>Guided GradCAM</b></center>')\n",
    "    grid[5:13, 1:9] = gc\n",
    "    grid[5:103, 11:19] = gbim\n",
    "    grid[5:13, 21:29] = ggc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_button(sender):\n",
    "    log_info(\"check_button\",\"enter function\")\n",
    "    if models.value == \"VanilaResNet50\":\n",
    "        model = load_VanilaResNet50()\n",
    "        gradCAM = GradCAM(model=model, layerName=\"conv5_block3_out\")\n",
    "        guidedBP = GuidedBackprop(model=model,layerName=\"conv5_block3_out\")\n",
    "        log_info(\"check_button\",\"set vanila function\")\n",
    "    elif models.value == \"ResNet50PlusFC\":\n",
    "        model = load_ResNet50PlusFC()\n",
    "        gradCAM = GradCAM(model=model, layerName=\"conv5_block3_out\")\n",
    "        guidedBP = GuidedBackprop(model=model, layerName=\"conv5_block3_out\")\n",
    "    \n",
    "    # read image\n",
    "    img = cv2.imread(os.path.join(SAMPLE_DIR,imgs.value))\n",
    "    log_info(\"check_button\",\"read sample image\")\n",
    "    upsample_size = (img.shape[1],img.shape[0])\n",
    "    x = preprocess(imgs.value)\n",
    "    log_info(\"check_button\",\"Completed image processing\")\n",
    "    pred, prob, predclasses, top_5_pred = predict(model,x)\n",
    "    class_ls = [\"--Select\"]\n",
    "    for c, p in zip(predclasses, top_5_pred):\n",
    "        class_ls.append(\"{} ({}) {:.3f}\".format(p[1], c, p[2]))\n",
    "        \n",
    "    CLASSES = widgets.Dropdown(options=class_ls, description=\"Class\", layout={'width':'auto'}, disabled=False)\n",
    "    grid[1,17:24] = CLASSES\n",
    "    log_info(\"check_button\",\"prediction completed\")\n",
    "    #log_info(\"check_button\",\"Checking classvalue {}/{}\".format(classes.value, INV_MAP[classes.value]))\n",
    "    if CLASSES.value == \"--Select\":\n",
    "        classIdx = pred\n",
    "    else:\n",
    "        classIdx = getclassidx(CLASSES.value)\n",
    "    \n",
    "    grid[2,9:18] = widgets.HTML(\"<center><span>Predicted Class: <b>{}<b> probability: <b>{:.3f}<b><span><center>\".format(pred, prob)) \n",
    "    log_info(\"check_button\",\"set grid\")\n",
    "    showCAMs(img, x, gradCAM, guidedBP, classIdx, upsample_size)\n",
    "    log_info(\"check_button\",\"completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getclassidx(value):\n",
    "    result = re.search(r\"\\[([0-9]+)\\]\", value)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_info(function=\"main\", message=\"\"):\n",
    "    if LOG:\n",
    "        print(\"{}\\t: {}\".format(function, message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_but.on_click(check_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ec230d957d43a6acd2f7863c95d8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridspecLayout(children=(HTML(value='<font color=\"#1f77b4\" face=\"sans-serif\"><center><h1>DEMO GradCAM and Guidâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the grid\n",
    "display(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradcam",
   "language": "python",
   "name": "gradcam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
